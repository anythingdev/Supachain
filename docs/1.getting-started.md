## Getting Started

[//]: # ({{tabs}})
[//]: # ({{tab}})
### 1. Set Up Local Providers
You can skip this step if your using remote models.

**Providers:** 
In the context of AI, providers are companies or organizations that offer AI models and services. 
Some popular providers include OpenAI, Gemini, and Gemma and LocalAI and Ollama for local models.

**Local Models:**
Run AI models locally for enhanced privacy, performance, and cost-effectiveness. You can use Ollama 
or LocalAI as providers. We also plan to release our own native provider in a future release.

**Benefits:**
- **Privacy:** Keep data local.
- **Performance:** Reduce latency.
- **Cost:** Avoid cloud fees.

### 1.1.0 Set Up Ollama

Ollama provides an easy way to run large language models on your system. Follow the steps below based on your operating system:

#### 1.1.1 Installation Instructions

- **macOS**: [Download Ollama](https://ollama.com/download/Ollama-darwin.zip)
- **Windows** (Preview): [Download Ollama](https://ollama.com/download/OllamaSetup.exe)
- **Linux**: Run the following command in your terminal:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

For manual installation instructions, visit the [Ollama GitHub page](https://github.com/ollama/ollama/blob/main/docs/linux.md).

You can also use the [official Docker image](https://hub.docker.com/r/ollama/ollama) by running:

```bash
docker run --name ollama -ti ollama/ollama
```

#### 1.1.2 Running Ollama on Windows GPU

To run Ollama on Windows with a NVIDIA GPU, you may need to set up the app using the NVIDIA Control Panel. Here’s a step-by-step guide:

1. **Download and Install Ollama:**
    - Use the [Ollama installer](https://ollama.com/download) for Windows.

   > **Tip: Set Up GPU Usage via NVIDIA Control Panel**
   > - Open the **NVIDIA Control Panel** (right-click on your desktop and select it from the context menu).
   > - Go to **Manage 3D Settings** > **Program Settings**.
   > - Add the **Ollama** app from your installation directory.
   > - Under **Select the preferred graphics processor**, choose **High-performance NVIDIA processor**.
   > - Apply the changes.

2. **Run Ollama:**
    - Once configured, you can run Ollama commands in your terminal, and it will leverage your NVIDIA GPU automatically.

Make sure your GPU drivers are up-to-date for optimal performance.

### 1.2.0 Set Up LocalAI

---
### 1.2.1. Install Docker

First, ensure Docker is installed on your machine:

- [Download Docker](https://www.docker.com/get-started) for your operating system.
- Follow the installation guide for your OS:
    - **[Windows](https://docs.docker.com/desktop/install/windows-install/)**
    - **[macOS](https://docs.docker.com/desktop/install/mac-install/)**
    - **[Linux](https://docs.docker.com/engine/install/)**

Once installed, verify Docker is running by entering the following command in your terminal:

```bash
docker --version
```

If Docker is installed, this command will display the version number.

---

#### 1.2.2 Check Your CUDA Version (Optional, for GPU Support)

If you plan to use LocalAI with NVIDIA GPUs, you'll need to check which CUDA version is installed on your system.

- Run the following command to check your CUDA version:

```bash
nvidia-smi
```

Look for the "CUDA Version" at the top of the output. It will show something like:

```
CUDA Version: 11.4
```


---

#### 1.2.3 Set up LocalAI (☕ May take a while)

Next, run LocalAI using Docker. The command depends on whether you have an NVIDIA, Intel, or AMD GPU, and which CUDA version you have.

**For systems without GPUs (CPU only):**

```bash
docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-cpu
```

[//]: # (Convert to tabs in the future)
**For systems with NVIDIA GPUs:**

1. **If your CUDA version is 11:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-nvidia-cuda-11
```

2. **If your CUDA version is 12:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-nvidia-cuda-12
```

**For systems with Intel GPUs:**

1. **If you need SYCL support (f16):**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-intel-f16
```

2. **If you need SYCL support (f32):**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-intel-f32
```

**For systems with AMD GPUs:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-hipblas
```

These commands will start the LocalAI container and map port 8080 on your local machine to the container.

---

#### 1.2.4 Download a Model

After starting the LocalAI container, you can download a model via the web interface.

1. **Open Your Web Browser** and go to:

   ```
   http://localhost:8080/browse/
   ```

2. **Browse and Select the Model**:
  - Navigate through the available models (e.g. find `meta-llama-3.1-8b-instruct`).
  - Click on the model name or corresponding download button to start the download.

3. **Wait for the Download to Complete**:
  - The download progress will be displayed on the web page.
  - Ensure the download is fully completed before closing the browser or stopping the Docker container.

This will download and make the selected model available for use with LocalAI.

---


## 2. Examples

### 2.1 A simple example using Ollama

**Example:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Answer
import dev.supachain.robot.provider.models.Ollama

// We want to call chat on our robot that outputs a true or false answer type
private interface TrueOrFalse {
    fun chat(prompt: String): Answer<Boolean>
}

fun main() {
    // Create a Robot instance specifying Ollama as the provider and TrueOrFalse as the answer type
    val robot = Robot<Ollama, TrueOrFalse, NoTools>()

    // Ask the question using the chat method
    val answer = robot.chat("The tallest building in the world is taller than Mt. Everest").await()

    // Print the answer
    println(answer)
}
```
#### Whats going on?

1. **Define an interface:** The `TrueOrFalse` interface specifies the expected format of the answer (a boolean value).
2. **Create a Robot instance:** The `Robot` instance is configured to use Ollama as the provider and expects `TrueOrFalse` answers.
3. **Ask a question:** The `chat` method is used to ask the question.
4. **Get the answer:** The `await()` method waits for the answer and returns it.

This example highlights how easy it is to use Supachain with Ollama. You can tailor the AI's responses 
to your specific needs by creating custom interfaces. The straightforward API makes integrating Supachain 
into your projects a breeze.

### 2.2 Customizing Your Local Provider
This example demonstrates how to configure Supachain to use a local AI provider with a specific URL 
and chat model.

**Code:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Chat
import dev.supachain.robot.provider.models.LocalAI

fun main() {
    val robot = Robot<LocalAI, Chat, NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            chatModel = "meta-llama-3.1-8b-instruct"
        }
    }

    val answer = robot.chat("Whats 11 * 7 + 1").await()
    println(answer)
}
```
#### Whats going on?
1. **Imports:** We import necessary libraries for Supachain, including `Robot`, the `Chat` interface for
   handling answers, and the `LocalAI` provider class.
2. **Robot with Configuration:** We create a `Robot` instance specifying `LocalAI` as the provider, `Chat` as the answer type, and `NoTools` indicating no additional tools are used.
3. **Customizing Provider:** The `defaultProvider` block allows us to configure the default local provider settings:
    - `url`: Sets the URL of the local AI service (replace with your actual URL). This is typically `http://localhost:<port>`, where `<port>` is the port your local AI service is running on (e.g., 8888 in this example).
    - `chatModel`: Specifies the specific chat model you want to use within the local AI provider (e.g., "meta-llama-3.1-8b-instruct").
4. **Ask a Question:** We use the `chat` method to ask the question "What's 11 * 7 + 1".
5. **Get the Answer:** The `await()` method waits for the answer from the local AI model and prints it.

**Benefits of Customization:**

- **Tailored Interactions:** Define the expected response format using interfaces like `Chat`.
- **Flexibility:** Specify the appropriate chat model based on your needs.
- **Control:** Set the URL to match your local AI service's location.

By understanding this customization approach, you can leverage Supachain effectively with various local AI providers and models.

### 2.3 Days of the Week Example with LocalAI

This example demonstrates how to configure Supachain to use a local AI provider to pick a specific day of the week 
based on the input prompt.

```kotlin
import dev.supachain.robot.Defaults
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Answer
import dev.supachain.robot.provider.models.LocalAI
import dev.supachain.utilities.Debug

enum class Days {
    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday
}

private interface DaysDirectives {
    fun pickADay(prompt: String): Answer<Days>
}

fun main() {
    Debug show "Messenger"

    val robot = Robot<LocalAI, DaysDirectives, Defaults.NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            temperature = 0.0
            chatModel = "LocalAI-llama3-8b-function-call-v0.2"
        }
    }

    robot.pickADay("In Japan many businesses are closed on this day of the week")
        .onAnswer { println(it) }
        .await()
}
```
---
### 2.4 Using Simple Math Operations in Supachain

This example demonstrates how to perform basic mathematical operations using tools in Supachain, with a focus on simple arithmetic and power calculations.

**Example:**

```kotlin
import dev.supachain.robot.Defaults.Chat
import dev.supachain.robot.Robot
import dev.supachain.robot.provider.models.Ollama
import dev.supachain.robot.tool.ToolSet
import dev.supachain.utilities.Debug
import kotlin.math.pow

interface SimpleMath {
    fun add(left: Double, right: Double): Double = left + right
    fun subtract(left: Double, right: Double): Double = left - right
    fun multiply(left: Double, right: Double): Double = left * right

    fun divide(left: Double, right: Double): Double {
        require(right != 0.0) { "Division by zero is not allowed" }
        return left / right
    }

    fun power(a: Double, b: Double): Double = a.pow(b)
}

@ToolSet
class SimpleMathExampleTools : SimpleMath

fun main() {
    Debug show "Messenger"

    val robot = Robot<Ollama, Chat, SimpleMathExampleTools>()

    val problem = "What is (16 * 16) / 2 - 2 ^ 2?"
    val answer = robot.chat(problem).await()
    println(answer)
}
```

#### Explanation:

1. **SimpleMath Interface:**
    - The `SimpleMath` interface provides basic arithmetic operations:
        - `add(left: Double, right: Double)`: Adds two numbers.
        - `subtract(left: Double, right: Double)`: Subtracts one number from another.
        - `multiply(left: Double, right: Double)`: Multiplies two numbers.
        - `divide(left: Double, right: Double)`: Divides one number by another, with a check to prevent division by zero.
        - `power(a: Double, b: Double)`: Raises a number to the power of another.

2. **ToolSet Implementation:**
    - The `SimpleMathExampleTools` class implements the `SimpleMath` interface and is annotated with `@ToolSet`, making it available as a toolset in Supachain.

3. **Robot Setup:**
    - A `Robot` instance is created using `Ollama` as the provider, `Chat` as the answer type, and `SimpleMathExampleTools` as the toolset.

4. **Problem Statement:**
    - The problem "What is (16 * 16) / 2 - 2 ^ 2?" is asked using the `chat` method.

5. **Get the Answer:**
    - The `await()` method waits for the AI to perform the calculation and returns the answer.

**Summary:**
This example shows how to integrate basic math operations into Supachain, enabling the AI to handle arithmetic tasks and provide accurate responses using the defined `SimpleMath` interface.
---
### 2.5 Using Tools in Supachain with LocalAI

This example demonstrates how to use tools in Supachain to enhance AI interactions by configuring a Local AI provider
with a specific tool strategy.

**Example:**

```kotlin
import dev.supachain.robot.Defaults.ChatMarkdown
import dev.supachain.robot.Robot
import dev.supachain.robot.provider.models.LocalAI
import dev.supachain.robot.tool.ToolSet
import dev.supachain.robot.tool.strategies.FillInTheBlank
import dev.supachain.utilities.Debug

typealias Coordinates = Array<Int>

@ToolSet
class Tools {
    fun getLongLat(city: String): Coordinates = arrayOf(0, 0)
    fun getWeather(coordinates: Coordinates): Double = 27.0
}

fun main() {
    Debug show "Messenger"

    val robot = Robot<LocalAI, ChatMarkdown, Tools> {
        defaultProvider {
            url = "http://localhost:8888"
            temperature = 0.0
            chatModel = "meta-llama-3.1-8b-instruct"
            toolStrategy = FillInTheBlank
        }
    }

    // Approximate Output: The weather in Toronto is 27 and in Tokyo is 27
    println(robot.chat("What's the weather in Tokyo and Toronto?").await()) 
}
```

#### Explanation:

1. **ToolSet Definition:**
    - A `ToolSet` named `Tools` is defined, which includes two functions:
        - `getLongLat(city: String)`: Returns coordinates (latitude and longitude) for a given city.
        - `getWeather(coordinates: Coordinates)`: Returns the weather based on coordinates.

2. **Robot Setup:**
    - A `Robot` instance is created using `LocalAI` as the provider, `ChatMarkdown` as the answer type, and `Tools` as the toolset.
    - The `FillInTheBlank` tool strategy is selected, where the AI fills in placeholders in a template response by calling corresponding tools.

3. **Ask a Question:**
    - The question "What's the weather in Tokyo and Toronto?" is asked using the `chat` method.

4. **Get the Answer:**
    - The `await()` method waits for the response, and the weather data for Tokyo and Toronto is printed.

**Tool Use Strategy:**

- **FillInTheBlank:** This strategy allows the AI to insert tool outputs directly into the response. For example, the AI can determine that it needs to call the `getLongLat` and `getWeather` functions to fill in the weather information for the given cities.

**Summary:**
This example illustrates how to extend Supachain with tools to provide more dynamic and context-aware responses, leveraging a local AI model with the `FillInTheBlank` strategy.
