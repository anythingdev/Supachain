## Getting Started

[//]: # ({{tabs}})
[//]: # ({{tab}})
### 1. Set Up Local Providers
You can skip this step if your using remote models.

**Providers:** 
In the context of AI, providers are companies or organizations that offer AI models and services. 
Some popular providers include OpenAI, Gemini, and Gemma and LocalAI and Ollama for local models.

**Local Models:**
Run AI models locally for enhanced privacy, performance, and cost-effectiveness. You can use Ollama 
or LocalAI as providers. We also plan to release our own native provider in a future release.

**Benefits:**
- **Privacy:** Keep data local.
- **Performance:** Reduce latency.
- **Cost:** Avoid cloud fees.

### 1.1.0 Set Up Ollama

Ollama provides an easy way to run large language models on your system. Follow the steps below based on your operating system:

#### 1.1.1 Installation Instructions

- **macOS**: [Download Ollama](https://ollama.com/download/Ollama-darwin.zip)
- **Windows** (Preview): [Download Ollama](https://ollama.com/download/OllamaSetup.exe)
- **Linux**: Run the following command in your terminal:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

For manual installation instructions, visit the [Ollama GitHub page](https://github.com/ollama/ollama/blob/main/docs/linux.md).

You can also use the [official Docker image](https://hub.docker.com/r/ollama/ollama) by running:

```bash
docker run --name ollama -ti ollama/ollama
```

#### 1.1.2 Running Ollama on Windows GPU

To run Ollama on Windows with a NVIDIA GPU, you may need to set up the app using the NVIDIA Control Panel. Here’s a step-by-step guide:

1. **Download and Install Ollama:**
    - Use the [Ollama installer](https://ollama.com/download) for Windows.

   > **Tip: Set Up GPU Usage via NVIDIA Control Panel**
   > - Open the **NVIDIA Control Panel** (right-click on your desktop and select it from the context menu).
   > - Go to **Manage 3D Settings** > **Program Settings**.
   > - Add the **Ollama** app from your installation directory.
   > - Under **Select the preferred graphics processor**, choose **High-performance NVIDIA processor**.
   > - Apply the changes.

2. **Run Ollama:**
    - Once configured, you can run Ollama commands in your terminal, and it will leverage your NVIDIA GPU automatically.

Make sure your GPU drivers are up-to-date for optimal performance.

### 1.2.0 Set Up LocalAI

---
### 1.2.1. Install Docker

First, ensure Docker is installed on your machine:

- [Download Docker](https://www.docker.com/get-started) for your operating system.
- Follow the installation guide for your OS:
    - **[Windows](https://docs.docker.com/desktop/install/windows-install/)**
    - **[macOS](https://docs.docker.com/desktop/install/mac-install/)**
    - **[Linux](https://docs.docker.com/engine/install/)**

Once installed, verify Docker is running by entering the following command in your terminal:

```bash
docker --version
```

If Docker is installed, this command will display the version number.

---

#### 1.2.2 Check Your CUDA Version (Optional, for GPU Support)

If you plan to use LocalAI with NVIDIA GPUs, you'll need to check which CUDA version is installed on your system.

- Run the following command to check your CUDA version:

```bash
nvidia-smi
```

Look for the "CUDA Version" at the top of the output. It will show something like:

```
CUDA Version: 11.4
```


---

#### 1.2.3 Set up LocalAI (☕ May take a while)

Next, run LocalAI using Docker. The command depends on whether you have an NVIDIA, Intel, or AMD GPU, and which CUDA version you have.

**For systems without GPUs (CPU only):**

```bash
docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-cpu
```

[//]: # (Convert to tabs in the future)
**For systems with NVIDIA GPUs:**

1. **If your CUDA version is 11:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-nvidia-cuda-11
```

2. **If your CUDA version is 12:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-nvidia-cuda-12
```

**For systems with Intel GPUs:**

1. **If you need SYCL support (f16):**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-intel-f16
```

2. **If you need SYCL support (f32):**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-intel-f32
```

**For systems with AMD GPUs:**

```bash
docker run -p 8080:8080 --gpus all --name local-ai -ti localai/localai:latest-gpu-hipblas
```

These commands will start the LocalAI container and map port 8080 on your local machine to the container.

---

#### 1.2.4 Download a Model

After starting the LocalAI container, you can download a model via the web interface.

1. **Open Your Web Browser** and go to:

   ```
   http://localhost:8080/browse/
   ```

2. **Browse and Select the Model**:
  - Navigate through the available models (e.g. find `meta-llama-3.1-8b-instruct`).
  - Click on the model name or corresponding download button to start the download.

3. **Wait for the Download to Complete**:
  - The download progress will be displayed on the web page.
  - Ensure the download is fully completed before closing the browser or stopping the Docker container.

This will download and make the selected model available for use with LocalAI.

---


## 2. Examples

### 2.1 A simple example using Ollama

**Example:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Answer
import dev.supachain.robot.provider.models.Ollama

// We want to call chat on our robot that outputs a true or false answer type
private interface TrueOrFalse {
    fun chat(prompt: String): Answer<Boolean>
}

fun main() {
    // Create a Robot instance specifying Ollama as the provider and TrueOrFalse as the answer type
    val robot = Robot<Ollama, TrueOrFalse, NoTools>()

    // Ask the question using the chat method
    val answer = robot.chat("The tallest building in the world is taller than Mt. Everest").await()

    // Print the answer
    println(answer)
}
```
#### Whats going on?

1. **Define an interface:** The `TrueOrFalse` interface specifies the expected format of the answer (a boolean value).
2. **Create a Robot instance:** The `Robot` instance is configured to use Ollama as the provider and expects `TrueOrFalse` answers.
3. **Ask a question:** The `chat` method is used to ask the question.
4. **Get the answer:** The `await()` method waits for the answer and returns it.

This example highlights how easy it is to use Supachain with Ollama. You can tailor the AI's responses 
to your specific needs by creating custom interfaces. The straightforward API makes integrating Supachain 
into your projects a breeze.

### 2.2 Customizing Your Local Provider
This example demonstrates how to configure Supachain to use a local AI provider with a specific URL 
and chat model.

**Code:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Chat
import dev.supachain.robot.provider.models.LocalAI

fun main() {
    val robot = Robot<LocalAI, Chat, NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            chatModel = "meta-llama-3.1-8b-instruct"
        }
    }

    val answer = robot.chat("Whats 11 * 7 + 1").await()
    println(answer)
}
```
#### Whats going on?
1. **Imports:** We import necessary libraries for Supachain, including `Robot`, the `Chat` interface for
   handling answers, and the `LocalAI` provider class.
2. **Robot with Configuration:** We create a `Robot` instance specifying `LocalAI` as the provider, `Chat` as the answer type, and `NoTools` indicating no additional tools are used.
3. **Customizing Provider:** The `defaultProvider` block allows us to configure the default local provider settings:
    - `url`: Sets the URL of the local AI service (replace with your actual URL). This is typically `http://localhost:<port>`, where `<port>` is the port your local AI service is running on (e.g., 8888 in this example).
    - `chatModel`: Specifies the specific chat model you want to use within the local AI provider (e.g., "meta-llama-3.1-8b-instruct").
4. **Ask a Question:** We use the `chat` method to ask the question "What's 11 * 7 + 1".
5. **Get the Answer:** The `await()` method waits for the answer from the local AI model and prints it.

**Benefits of Customization:**

- **Tailored Interactions:** Define the expected response format using interfaces like `Chat`.
- **Flexibility:** Specify the appropriate chat model based on your needs.
- **Control:** Set the URL to match your local AI service's location.

By understanding this customization approach, you can leverage Supachain effectively with various local AI providers and models.

### 2.3 Days of the Week Example with LocalAI

This example demonstrates how to configure Supachain to use a local AI provider to pick a specific day of the week 
based on the input prompt.

```kotlin
enum class Days {
    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday
}

private interface DaysDirectives {
    fun pickADay(prompt: String): Answer<Days>
}

fun main() {
    Debug show "Messenger"

    val robot = Robot<LocalAI, DaysDirectives, Defaults.NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            temperature = 0.0
            chatModel = "LocalAI-llama3-8b-function-call-v0.2"
        }
    }

    robot.pickADay("In Japan many businesses are closed on this day of the week")
        .onAnswer { println(it) }
        .await()
}
```

Here’s how you can add the new example to the existing section of your wiki:

---

## 2. Examples

### 2.1 A simple example using Ollama

**Example:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Answer
import dev.supachain.robot.provider.models.Ollama

// We want to call chat on our robot that outputs a true or false answer type
private interface TrueOrFalse {
    fun chat(prompt: String): Answer<Boolean>
}

fun main() {
    // Create a Robot instance specifying Ollama as the provider and TrueOrFalse as the answer type
    val robot = Robot<Ollama, TrueOrFalse, NoTools>()

    // Ask the question using the chat method
    val answer = robot.chat("The tallest building in the world is taller than Mt. Everest").await()

    // Print the answer
    println(answer)
}
```

#### What's going on?

1. **Define an interface:** The `TrueOrFalse` interface specifies the expected format of the answer (a boolean value).
2. **Create a Robot instance:** The `Robot` instance is configured to use Ollama as the provider and expects `TrueOrFalse` answers.
3. **Ask a question:** The `chat` method is used to ask the question.
4. **Get the answer:** The `await()` method waits for the answer and returns it.

This example highlights how easy it is to use Supachain with Ollama. You can tailor the AI's responses to your specific needs by creating custom interfaces. The straightforward API makes integrating Supachain into your projects a breeze.

### 2.2 Customizing Your Local Provider

This example demonstrates how to configure Supachain to use a local AI provider with a specific URL and chat model.

**Code:**

```kotlin
import dev.supachain.robot.Defaults.*
import dev.supachain.robot.Robot
import dev.supachain.robot.answer.Chat
import dev.supachain.robot.provider.models.LocalAI

fun main() {
    val robot = Robot<LocalAI, Chat, NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            chatModel = "meta-llama-3.1-8b-instruct"
        }
    }

    val answer = robot.chat("What's 11 * 7 + 1").await()
    println(answer)
}
```

#### What's going on?

1. **Imports:** We import necessary libraries for Supachain, including `Robot`, the `Chat` interface for handling answers, and the `LocalAI` provider class.
2. **Robot with Configuration:** We create a `Robot` instance specifying `LocalAI` as the provider, `Chat` as the answer type, and `NoTools` indicating no additional tools are used.
3. **Customizing Provider:** The `defaultProvider` block allows us to configure the default local provider settings:
    - `url`: Sets the URL of the local AI service (replace with your actual URL). This is typically `http://localhost:<port>`, where `<port>` is the port your local AI service is running on (e.g., 8888 in this example).
    - `chatModel`: Specifies the specific chat model you want to use within the local AI provider (e.g., "meta-llama-3.1-8b-instruct").
4. **Ask a Question:** We use the `chat` method to ask the question "What's 11 * 7 + 1".
5. **Get the Answer:** The `await()` method waits for the answer from the local AI model and prints it.

**Benefits of Customization:**

- **Tailored Interactions:** Define the expected response format using interfaces like `Chat`.
- **Flexibility:** Specify the appropriate chat model based on your needs.
- **Control:** Set the URL to match your local AI service's location.

By understanding this customization approach, you can leverage Supachain effectively with various local AI providers and models.

---

### 2.3 Days of the Week Example with LocalAI

This example demonstrates how to configure Supachain to use a local AI provider to pick a specific day of the week based on the input prompt.

**Code:**

```kotlin
enum class Days {
    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday
}

private interface DaysDirectives {
    fun pickADay(prompt: String): Answer<Days>
}

fun main() {
    Debug show "Messenger"

    val robot = Robot<LocalAI, DaysDirectives, Defaults.NoTools> {
        defaultProvider {
            url = "http://localhost:8888"
            temperature = 0.0
            chatModel = "LocalAI-llama3-8b-function-call-v0.2"
        }
    }

    robot.pickADay("In Japan many businesses are closed on this day of the week")
        .onAnswer { println(it) }
        .await()
}
```

#### What's going on?

1. **Enum Definition:** The `Days` enum lists all the days of the week.
2. **Define an Interface:** The `DaysDirectives` interface is created with a method `pickADay` that returns an answer of type `Days`.
3. **Create a Robot Instance:** We create a `Robot` instance specifying `LocalAI` as the provider, `DaysDirectives` as the directive interface type, and `NoTools` indicating no additional tools are used.
4. **Customize the Provider:** The `defaultProvider` block configures the provider settings, including setting the URL, temperature, and chat model.
5. **Pick a Day:** We use the `pickADay` method to ask the question, and the answer is printed.

**Learning Points:**
- **Custom Interfaces:** The `DaysDirectives` interface is customized to return specific types, such as `Days`, based on the prompt.
- **Provider Flexibility:** The local provider is customized for this specific example, demonstrating how to fine-tune the model's behavior.

This example shows how you can tailor the AI model to handle specific tasks, like selecting a day of the week, based on your application's needs.